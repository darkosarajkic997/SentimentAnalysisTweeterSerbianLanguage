{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600839707364",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classla\n",
    "import ctypes\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<CDLL 'caffe2_nvrtc.dll', handle 7ffa9e850000 at 0x1dad2163520>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "ctypes.cdll.LoadLibrary('caffe2_nvrtc.dll')"
   ]
  },
  {
   "source": [
    "## Get lemmas from conllu format as an array"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(conll):\n",
    "    res=[]\n",
    "    for line in conll.splitlines():\n",
    "        if not line.startswith('#'):\n",
    "            words = line.strip('\\n').split('\\t')\n",
    "            if(len(words)>3):  \n",
    "                res.append(words[2])\n",
    "    return res"
   ]
  },
  {
   "source": [
    "## Insert negation marker NEG"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_negation(lemma_array, text, negation_tokens, characters):\n",
    "    res=[]\n",
    "    words=text.split()\n",
    "    index=0\n",
    "    \n",
    "    for lemma in lemma_array:\n",
    "        if lemma not in characters:\n",
    "            orginial_word=words[index]\n",
    "            index+=1\n",
    "\n",
    "            if orginial_word in negation_tokens:\n",
    "                res.append('NEG')\n",
    "\n",
    "        res.append(lemma)\n",
    "    \n",
    "    return ' '.join(res)      \n",
    "    "
   ]
  },
  {
   "source": [
    "## Find lemma for every word and set NEG markers for negation words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization_and_negation(tweets_dataframe, negation_words, nlp):\n",
    "    \n",
    "    negation_tokens=negation_words.split()\n",
    "    characters=\".,!?'\"\n",
    "    translator=str.maketrans(characters, ' '*len(characters)) \n",
    "    tweets_dataframe['FilteredText']=''\n",
    "    for index,row in tweets_dataframe.iterrows():\n",
    "        doc=nlp(row['Text'])\n",
    "        lemma_array=get_lemmas(doc.conll_file.conll_as_string())\n",
    "        text=row['Text'].translate(translator)\n",
    "        row['FilteredText']=filter_negation(lemma_array,text,negation_tokens,characters)\n",
    "    return tweets_dataframe\n"
   ]
  },
  {
   "source": [
    "## loop through tweets in blocks of 200 tweets and performe lemmatization and negation on tweet text"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Use device: gpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_tokenizer.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: ner\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_ner.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'forward_charlm_path': None, 'backward_charlm_path': None, 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: pos\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: lemma\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_lemmatizer.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nUsing POS in encoder\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\nDone loading processors!\n---\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n"
    }
   ],
   "source": [
    "negation_words='nema nemamo neću ni necete nemate nemoj nećavši nisam necemo nemaš nemojmo nisu nisi neces nemaju neće niste stop necu nemam nećete nismo necavsi nemojte nećemo nije nece nit niti nećeš ne nemas'\n",
    "\n",
    "tweets=pd.read_pickle(\"PickleDataframes\\\\Processed_Tweets.pkl\")\n",
    "number_of_tweets=len(tweets.index)\n",
    "step=200\n",
    "index=0\n",
    "pok=0\n",
    "nlp = classla.Pipeline('sr', type='nonstandard')\n",
    "\n",
    "while index < number_of_tweets:\n",
    "    top=min(index+step,number_of_tweets)\n",
    "    df=tweets.iloc[index:top]\n",
    "    new_df=lemmatization_and_negation(df, negation_words,nlp)\n",
    "    file_name_pkl=''.join(['LemmaNegation\\\\LemmaTweetsData',str(pok),'.pkl'])\n",
    "    new_df.to_pickle(file_name_pkl)\n",
    "    print(pok)\n",
    "    index+=step\n",
    "    pok+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}