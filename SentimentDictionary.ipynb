{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600870173235",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classla\n",
    "import ctypes\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<CDLL 'caffe2_nvrtc.dll', handle 7ffa78e40000 at 0x20203a867f0>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "ctypes.cdll.LoadLibrary('caffe2_nvrtc.dll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(conll):\n",
    "    res=[]\n",
    "    for line in conll.splitlines():\n",
    "        if not line.startswith('#'):\n",
    "            words = line.strip('\\n').split('\\t')\n",
    "            if(len(words)>3):  \n",
    "                res.append(words[2])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(array1, array2): \n",
    "    array3 = [value for value in array1 if value in array2] \n",
    "    return array3\n",
    "\n",
    "def unique_array(array1, array2):\n",
    "    array3=[value for value in array1 if value not in array2]\n",
    "    return array3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Use device: gpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_tokenizer.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: ner\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_ner.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'forward_charlm_path': None, 'backward_charlm_path': None, 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: pos\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\n---\nLoading: lemma\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_nonstandard_models\\\\sr_nonstandard_lemmatizer.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nUsing POS in encoder\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings: \n{'model_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\DarkoSarajkic\\\\classla_resources\\\\sr_set_models\\\\sr_set.pretrain.pt', 'lang': 'sr', 'shorthand': 'sr_nonstandard', 'mode': 'predict', 'type': 'nonstandard'}\nDone loading processors!\n---\n"
    }
   ],
   "source": [
    "nlp = classla.Pipeline('sr', type='nonstandard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "pos_words_lemmas=[]\n",
    "\n",
    "f = open('TxtDocs\\\\PositiveWordsDict.txt','r', encoding='utf8')\n",
    "for word in f.read().split():\n",
    "    words.append(word)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_count=len(words)\n",
    "index=0\n",
    "while index<word_count:\n",
    "    top=min(index+20,word_count)\n",
    "    words_to_process=' '.join(words[index:top])\n",
    "    index+=20\n",
    "    doc = nlp(words_to_process)\n",
    "    pos_words_lemmas+=get_lemmas(doc.conll_file.conll_as_string())\n",
    "\n",
    "pos_words_lemmas=sorted(list(set(pos_words_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words_lemmas=[]\n",
    "words=[]\n",
    "\n",
    "f = open('TxtDocs\\\\NegativeWordsDict.txt','r',encoding='utf8')\n",
    "for word in f.read().split():\n",
    "    words.append(word)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count=len(words)\n",
    "index=0\n",
    "while index<word_count:\n",
    "    top=min(index+20,word_count)\n",
    "    words_to_process=' '.join(words[index:top])\n",
    "    index+=20\n",
    "    doc = nlp(words_to_process)\n",
    "    neg_words_lemmas+=get_lemmas(doc.conll_file.conll_as_string())\n",
    "\n",
    "\n",
    "neg_words_lemmas=sorted(list(set(neg_words_lemmas)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_lemmas=intersection(pos_words_lemmas,neg_words_lemmas)\n",
    "pos_words_lemmas=unique_array(pos_words_lemmas,mutual_lemmas)\n",
    "neg_words_lemmas=unique_array(neg_words_lemmas,mutual_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open('TxtDocs\\\\PositiveSentimentWords.txt', 'w+',encoding='utf8')\n",
    "n = text_file.write('\\n'.join(pos_words_lemmas))\n",
    "text_file.close()\n",
    "\n",
    "text_file = open('TxtDocs\\\\NegativeSentimentWords.txt', 'w+',encoding='utf8')\n",
    "n = text_file.write('\\n'.join(neg_words_lemmas))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}